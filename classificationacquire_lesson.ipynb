{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=limegreen>Acquire Data for Classification</font>\n",
    "\n",
    "**A Few Example Methods for Reading Data into Pandas DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Ideas\n",
    "\n",
    "- Cache your data to speed up your data acquisition.\n",
    "\n",
    "- Helper functions are your friends.\n",
    "\n",
    "# Objectives \n",
    "\n",
    "**By the end of the acquire lesson and exercises, you will be able to...**\n",
    "\n",
    "- **read data into a pandas DataFrame using the following modules:**\n",
    "\n",
    ">pydataset\n",
    "    \n",
    "```python\n",
    "from pydataset import data\n",
    "df = data('dataset_name')\n",
    "```\n",
    ">seaborn datasets\n",
    "    \n",
    "```python\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('dataset_name')\n",
    "```\n",
    "\n",
    "- **read data into a pandas DataFrame from the following sources:**\n",
    "\n",
    "    - an Excel spreadsheet\n",
    "\n",
    "    - a Google sheet\n",
    "    \n",
    "    - Codeup's mySQL database\n",
    "\n",
    "```python\n",
    "pd.read_excel('file_name.xlsx', sheet_name='sheet_name')\n",
    "pd.read_csv('filename.csv')\n",
    "pd.read_sql(sql_query, connection_url)\n",
    "```\n",
    "\n",
    "- **use pandas methods and attributes to do some initial summarization and exploration of your data.**\n",
    "\n",
    "```python\n",
    ".head()\n",
    ".shape\n",
    ".info()\n",
    ".columns\n",
    ".dtypes\n",
    ".describe()\n",
    ".value_counts()\n",
    "```\n",
    "\n",
    "- **create functions that acquire data from Codeup's database, save the data locally to CSV files (cache your data), and check for CSV files upon subsequent use.**\n",
    "\n",
    "\n",
    "- **create a new python module, `acquire.py`, that holds your functions that acquire the titanic and iris data and can be imported and called in other notebooks and scripts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6cd8c8b87a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# acquire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'env'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(11, 9))\n",
    "plt.rc('font', size=13)\n",
    "\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# acquire\n",
    "from env import host, user, password\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a Database\n",
    "\n",
    "Create your DataFrame using a SQL query to access a database.\n",
    "\n",
    "**<font color=purple>Use your env file info and create your sql query and create connection_url for use in pandas `read_sql()` function.</font>**\n",
    "\n",
    "```python\n",
    "# Import private info to keep it secret in public files.\n",
    "from env import host, password, user\n",
    "\n",
    "# Test query in Sequel Pro and save to a variable.\n",
    "sql_query = 'write your sql query here; test it in Sequel Pro first!'\n",
    "\n",
    "# Save connection url to a variable for use with pandas `read_sql()` function.\n",
    "connection_url = f'mysql+pymysql://{user}:{password}@{host}/database_name'\n",
    "    \n",
    "# Python function to read data from database into a DataFrame.\n",
    "pd.read_sql(sql_query, connection_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql query and save to variable.\n",
    "\n",
    "sql_query = 'SELECT * FROM passengers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection url and save to a variable.\n",
    "\n",
    "connection_url = f'mysql+pymysql://{user}:{password}@{host}/titanic_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use my variables in the pandas read_sql() function.\n",
    "\n",
    "titanic_df = pd.read_sql(sql_query, connection_url)\n",
    "titanic_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Files\n",
    "\n",
    "- Create your DataFrame from a csv file.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('file_path/file_name.csv')\n",
    "```\n",
    "- Create your DataFrame from an AWS S3 file.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('https://s3.amazonaws.com/bucket_and_or_file_name.csv')\n",
    "```\n",
    "\n",
    "- Create your DataFrame from a Google sheet using its Share url.\n",
    "\n",
    "```python\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'\n",
    "```  \n",
    "\n",
    "```python\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "```\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(csv_export_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our Google Sheet share url to a variable.\n",
    "\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the replace method to modify our Google Sheet share url to be a csv export url.\n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_csv() method to create our DataFrame.\n",
    "\n",
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Your Clipboard\n",
    "\n",
    "Read copy-pasted tabular data and parse it into a DataFrame.\n",
    "\n",
    "```python\n",
    "# Default\n",
    "df = pd.read_clipboard(sep='\\\\s+', **kwargs)\n",
    "\n",
    "# Some examples of options I have.\n",
    "columns = ['column_1', 'column_2', 'column_3']\n",
    "df = pd.read_clipboard(sep=',', header=None, names=columns)\n",
    "```\n",
    "\n",
    "[Here's](https://towardsdatascience.com/pandas-hacks-read-clipboard-94a05c031382) a short and sweet article that explains it all nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the read_clipboard() method here using the article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the read_clipboard() method with data without headers/column names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From an Excel Sheet\n",
    "\n",
    "```python\n",
    "pd.read_excel('your_excel_file_name.xlsx', sheet_name='your_table_name', usecols=['this_one', 'this_one'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one sheet from my_telco_churn excel workbook.\n",
    "\n",
    "customers_df = pd.read_excel('my_telco_churn.xlsx', sheet_name='Table2_CustDetails')\n",
    "customers_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Pydataset\n",
    "\n",
    "Create your DataFrame using Pydataset and Read the Doc.\n",
    "\n",
    "```python\n",
    "from pydataset import data\n",
    "\n",
    "data('iris', show_doc=True)\n",
    "\n",
    "df_iris = data('iris')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame using pydataset 'iris'\n",
    "\n",
    "df_iris = data('iris')\n",
    "df_iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Seaborn Datasets. This one has nice column names! :)\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Data Acquisition\n",
    "\n",
    "-  The process of acquiring, preparing, exploring, modeling, and evaluating data is called the Data Science Pipeline.\n",
    "\n",
    "\n",
    "- As we go through the pipeline, our goal is to end each stage with functions that automate the process and can feed into the next stage, making our work faster and more importantly, repeatable.\n",
    "\n",
    "\n",
    "- We store our functions from each stage in modules, `acquire.py`, `prepare.py`, etc., and import them for use in our notebooks. All of the helper and main functions are stored in the `.py` file or module to keep our notebook clean and readable.\n",
    "\n",
    "\n",
    "- Ideally, upon completing the entire process, we should be able to use all of our functions, from each stage, to create one pipeline function that can reproduce our entire process from aquisition to evaluation.\n",
    "\n",
    "\n",
    "- If our goal is to acquire the titanic data from the Codeup database, both of the funtions below would be stored in an `acquire.py` file and imported into our notebook for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=purple>Put it all together in a single function that acquires new data from the Codeup database and save it, as well as any helper functions, in your `acquire.py` file.</font>**\n",
    "\n",
    "```python\n",
    "# Create helper function to get the necessary connection url.\n",
    "def get_connection(db, user=user, host=host, password=password):\n",
    "    '''\n",
    "    This function uses my info from my env file to\n",
    "    create a connection url to access the Codeup db.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "# Use the above helper function and a sql query in a single function.\n",
    "def get_db_data():\n",
    "    '''\n",
    "    This function reads data from the Codeup db into a df.\n",
    "    '''\n",
    "    sql_query = 'write your sql query here; test it in Sequel Pro first!'\n",
    "    return pd.read_sql(sql_query, get_connection('database_name'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a helper function that creates our connection url.\n",
    "\n",
    "def get_connection(db, user=user, host=host, password=password):\n",
    "    '''\n",
    "    This function uses my info from my env file to\n",
    "    create a connection url to access the Codeup db.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_titanic_data():\n",
    "    '''\n",
    "    This function reads in the titanic data from the Codeup db\n",
    "    and returns a pandas DataFrame with all columns.\n",
    "    '''\n",
    "    sql_query = 'SELECT * FROM passengers'\n",
    "    return pd.read_sql(sql_query, get_connection('titanic_db'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Data\n",
    "\n",
    "**<font color=green>Save time by saving your data to a csv file for future use.</font>**\n",
    "\n",
    "- Caching or storing data you've retrieved from a database or website makes accessing it later much faster. Basically, cached data reduces load times.\n",
    "\n",
    "- We can design our acquire functions to get our data for us faster by reading in a csv file, if one exists, and if not, acquiring our data and creating a csv file for later use.\n",
    "\n",
    "- The `os.path.isfile()` method in Python is used to check whether a specified path is an existing file or not. It returns a boolean value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 10px groove limegreen; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check to see if a file names 'titanic_df.csv' exists in this directory.\n",
    "\n",
    "os.path.isfile('titanic_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write our 'titanic_df' DataFrame to a csv file.\n",
    "\n",
    "titanic_df.to_csv('titanic_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check again...\n",
    "\n",
    "os.path.isfile('titanic_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use this concept to write a new function that allows us to hit the Codeup database, write the data to a csv file for later use, and read the data into a pandas DataFrame the next time we call the function and the csv file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our first helper function that's used below.\n",
    "\n",
    "def get_connection(db, user=user, host=host, password=password):\n",
    "    '''\n",
    "    This function uses my info from my env file to\n",
    "    create a connection url to access the Codeup db.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our new_titanic_data() function from above as a helper in a final function.\n",
    "\n",
    "def new_titanic_data():\n",
    "    '''\n",
    "    This function reads the titanic data from the Codeup db into a df,\n",
    "    write it to a csv file, and returns the df.\n",
    "    '''\n",
    "    # Create SQL query.\n",
    "    sql_query = 'SELECT * FROM passengers'\n",
    "    \n",
    "    # Read in DataFrame from Codeup db.\n",
    "    df = pd.read_sql(sql_query, get_connection('titanic_db'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titanic_data(cached=False):\n",
    "    '''\n",
    "    This function reads in titanic data from Codeup database and writes data to\n",
    "    a csv file if cached == False or if cached == True reads in titanic df from\n",
    "    a csv file, returns df.\n",
    "    '''\n",
    "    if cached == False or os.path.isfile('titanic_df.csv') == False:\n",
    "        \n",
    "        # Read fresh data from db into a DataFrame.\n",
    "        df = new_titanic_data()\n",
    "        \n",
    "        # Write DataFrame to a csv file.\n",
    "        df.to_csv('titanic_df.csv')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If csv file exists or cached == True, read in data from csv.\n",
    "        df = pd.read_csv('titanic_df.csv', index_col=0)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_titanic_data()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_titanic_data(cached=False)\n",
    "df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
